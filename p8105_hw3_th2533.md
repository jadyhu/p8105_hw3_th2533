Homework 3
================
Tianheng Hu

# Problem 1

``` r
data("instacart")
```

This dataset contains 1384617 rows and 15 columns.

Observations are the level of items in orders by user. There are
user/order variables – user ID, order ID, order day, and order hour.
There are also item variables – name, aisle, department, and some
numeric codes.

How many aisles, and which are most items from?

``` r
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

    ## # A tibble: 134 x 2
    ##    aisle                              n
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # … with 124 more rows

Let’s make a plot.

``` r
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x=element_text(angle = 90, vjust = 0.5, hjust = 1))
```

<img src="p8105_hw3_th2533_files/figure-gfm/unnamed-chunk-3-1.png" width="90%" />

Let’s make a table

``` r
instacart %>% 
  filter(aisle %in% c ("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```

| aisle                      | product\_name                                 |    n | rank |
| :------------------------- | :-------------------------------------------- | ---: | ---: |
| baking ingredients         | Light Brown Sugar                             |  499 |    1 |
| baking ingredients         | Pure Baking Soda                              |  387 |    2 |
| baking ingredients         | Cane Sugar                                    |  336 |    3 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |   30 |    1 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |   28 |    2 |
| dog food care              | Small Dog Biscuits                            |   26 |    3 |
| packaged vegetables fruits | Organic Baby Spinach                          | 9784 |    1 |
| packaged vegetables fruits | Organic Raspberries                           | 5546 |    2 |
| packaged vegetables fruits | Organic Blueberries                           | 4966 |    3 |

Apples vs Ice cream

``` r
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarise(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```

    ## `summarise()` regrouping output by 'product_name' (override with `.groups` argument)

    ## # A tibble: 2 x 8
    ## # Groups:   product_name [2]
    ##   product_name       `0`   `1`   `2`   `3`   `4`   `5`   `6`
    ##   <chr>            <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
    ## 1 Coffee Ice Cream  13.8  14.3  15.4  15.3  15.2  12.3  13.8
    ## 2 Pink Lady Apples  13.4  11.4  11.7  14.2  11.6  12.8  11.9

# Problem 2

Load, tidy and wrangle the dataset

``` r
accel = 
  read_csv(
  "./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  select(-day_id) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity_count"
  ) %>% 
  mutate(
    weekday = ifelse(day %in% c("Saturday","Sunday"), FALSE, TRUE),
    minute = as.numeric(minute),
    day = factor(day, levels= c("Monday", "Tuesday", "Wednesday", "Thursday", 
                      "Friday", "Saturday", "Sunday"))
  ) %>% 
  arrange(day)
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_double(),
    ##   day = col_character()
    ## )

    ## See spec(...) for full column specifications.

This dataset contains 50400 rows and 5 columns.

Observations are the level of activities on a 63 year-old male with BMI
25 in 5 weeks. There are 50400 observations in total. There are time
variables – week,day and whether it is weekday or weekend. There are
also activity variables –number of activity and activity count.

Total activity for each day

``` r
  accel %>% 
  group_by(week, day) %>% 
  summarise(total_activity = sum(activity_count)) %>% 
  pivot_wider(
    names_from = day,
    values_from = total_activity
  ) %>% 
  knitr::kable(digits = 0)
```

    ## `summarise()` regrouping output by 'week' (override with `.groups` argument)

| week | Monday | Tuesday | Wednesday | Thursday | Friday | Saturday | Sunday |
| ---: | -----: | ------: | --------: | -------: | -----: | -------: | -----: |
|    1 |  78828 |  307094 |    340115 |   355924 | 480543 |   376254 | 631105 |
|    2 | 295431 |  423245 |    440962 |   474048 | 568839 |   607175 | 422018 |
|    3 | 685910 |  381507 |    468869 |   371230 | 467420 |   382928 | 467052 |
|    4 | 409450 |  319568 |    434460 |   340291 | 154049 |     1440 | 260617 |
|    5 | 389080 |  367824 |    445366 |   549658 | 620860 |     1440 | 138421 |

From the table of total\_activity of each days in five weeks, we can
find some patterns. For week 1 and week 2, the person is very active on
weekends and not active on weekdays, especially Monday. Starting from
week 3, the person has some changes in his life. He becomes more active
on weekdays and less active on weekends. The range of total activities
is also very big. For week 5, the person is very active on Friday but
extremely inactive on Saturday. In addition, this person has a generally
low activity level in week 4.

Let’s plot the activity in a day.

``` r
accel %>% 
  ggplot(aes(x = minute, y = activity_count, color = day)) +
  geom_line(alpha = 0.5) +
  labs(
    title = "Activity plot",
    x = "Minutes in a day",
    y = "Activity count",
    caption = "Data from the accelerometer"
  ) 
```

<img src="p8105_hw3_th2533_files/figure-gfm/unnamed-chunk-8-1.png" width="90%" />

From the plot of activity count over the course of the day, we can see
that this person tends to workout in the evening of a day from the
cluster of high activity level from 1000 mins to 1300 mins. On weekends,
typically Sunday, he likes to workout in the middle of the day. On
Saturdays, his activities are distributed throughout the day. Combining
with the information from the table, this person don’t have a stable
activity level everyday in a week. He usually has workout days when he
normally has higher activity level and has rest days when he normally
has lower activity level. The workout days and rest days change throught
different weeks.

# Problem 3

``` r
data("ny_noaa")
```

This dataset contains 2595176 rows and 7 columns. This documents the
weather data from Global Historical Climatology Network.

Observations are the weather conditions from all New York State weather
stations from January 1, 1981 through December 31, 2010 There are
2595176 observations in total. There are station/date variables –
weather station id and date of observation. There are also weather
condition variables –precipitation, snowfall, snow depth, Maximum and
minimum temperature. In the dataset, there are total of
`mean(is.na(ny_noaa))*100`% missing values. Maximum and minimum
temperature have the most missing values about
`mean(is.na(ny_noaa$tmax))*100`%. So missing values are a big problem
when working with this dataset.

Tidy the dataset

``` r
ny_noaa = 
  ny_noaa %>% 
  separate(date, c("year", "month", "day")) %>% 
  mutate(
    month = as.integer(month),
    year = as.integer(year),
    month = month.name[month],
    tmax = as.numeric(tmax),
    tmax = tmax/10,
    tmin = as.numeric(tmin),
    tmin = tmin/10,
    prcp = prcp/10) 
```

Look at snowfall specifically.

``` r
ny_noaa %>% 
  count(snow) %>% 
  arrange(desc(n)) %>% 
  slice_head(n = 1) %>% 
  knitr::kable()
```

| snow |       n |
| ---: | ------: |
|    0 | 2008508 |

Zero snowfall is the most commonly observed value. This makes sense
because it is winter for only a few months with snow in New York. Most
time there is no snow in NY.

Let’s make a two panel plot showing the average max temperature in
January and in July in each station across years.

``` r
ny_noaa %>% 
  filter(month %in% c("January", "July")) %>% 
  group_by(year, id, month) %>%
  summarise(avg_max = mean(tmax)) %>% 
  ggplot(aes(x = year, y = avg_max, color = month)) +
  geom_point() +
  geom_smooth(se = FALSE) + 
  facet_grid(. ~ month) +
  labs(
    title = "Average Max Temperature plot",
    x = "Year from 1981 to 2010",
    y = "Average Max Temperature",
    caption = "Data from the ny_noaa"
  ) 
```

    ## `summarise()` regrouping output by 'year', 'id' (override with `.groups` argument)

    ## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'

    ## Warning: Removed 7058 rows containing non-finite values (stat_smooth).

    ## Warning: Removed 7058 rows containing missing values (geom_point).

<img src="p8105_hw3_th2533_files/figure-gfm/unnamed-chunk-12-1.png" width="90%" />

The average maximum temperatures in July from 1981 to 2010 don’t change
a lot and they are constant across the 30 years. More fluctuations are
observed in the average maximum temperature in January from 1981 to
2010. The fluctuations are in a range of 5 degrees. There are 2 obvious
outliers in January temperatures and a few more in July temperatures.
From the maximum temperatures in July in these 30 years, we don’t
observe a general trend of global warming, ie. increasing temperatures.
However, there is a trend of global warming between 1980 and 1990 in max
temperature in January and the fluctuations in max temperatures may
suggest that our climate is becoming more unpredictable and possibly an
indication of global warming effect.

Let’s make a two panel plot

``` r
tmax_tmin_p = 
ny_noaa %>% 
  ggplot(aes(x = tmin, y = tmax)) + 
  geom_hex() +
  labs(
    title = "Distributon of max and min temperature",
    x = "Min temperature",
    y = "Max temperature"
  ) +
  theme(legend.position = "right") 

snowfall_dist_p =
ny_noaa %>% 
  filter(snow > 0 & snow < 100) %>% 
  mutate(year = factor(year)) %>% 
  ggplot(aes(x = year, y = snow)) + 
  geom_boxplot() +
  labs(
    title = "Distributon of snowfall between 0 and 100 plot",
    x = "Year from 1981 to 2010",
    y = "Snowfall"
  ) +
  theme(legend.position = "right")

tmax_tmin_p / snowfall_dist_p
```

    ## Warning: Removed 1136276 rows containing non-finite values (stat_binhex).

<img src="p8105_hw3_th2533_files/figure-gfm/unnamed-chunk-13-1.png" width="90%" />

From the hex plot, we can see most observations are around the center of
the distribution. There are som variabilities. Maximum temperature and
minimum temperature has a linear relationship. Higher minimun
temperature corresponds to a higher maximum temperature. This makes
sense.

From the boxplots of the distribution of snowfall between 0 and 100, we
can see that the distributions are similar across 1981 to 2010. They are
all a right-skewed. In 1998, 2006 and 2010, there are several outliers
in the snowfall distribution. Most snowfall is within a range of 20mm to
50mm for most years.In 2006, there is generally less snowfall but some
extremely heavy snowfall.
